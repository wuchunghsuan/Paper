\section{Background and Observations}\label{motivation}

In this section, we first study the typical shuffle characteristics (\ref{shuffle pattern}), and then spot the opportunities to achieve shuffle optimization (\ref{observation}).
\subsection{Characteristic of Shuffle} \label{shuffle pattern}

{\color{black}
In large scale data parallel computing, shuffle is designed to achieve an all-to-all data transfer among nodes.
The shuffle process can be further split into two parts: \textit{shuffle write} and \textit{shuffle read}. 
Shuffle write starts at the end of a map task and writes the partitioned map output data to local persistent storage. 
Shuffle read starts at the beginning of a reduce task and fetches the partitioned data from remote as its input.
Thus, Shuffle is I/O intensive, which might introduce a significant latency to the application. 
Reports show that $60\%$ of MapReduce jobs at Yahoo! and $20\%$ at Facebook are shuffle-heavy workloads \cite{shufflewatcher}. 
For those shuffle-heavy jobs, the shuffle latency may even dominate the completion time of jobs.
}
% For a clear illustration, we use \textit{map tasks} to define the tasks that produce shuffle data and use \textit{reduce tasks} to define the tasks that consume shuffle data.
% Note that one task may have both shuffle data generation and consumption in modern DAG framework. These tasks contain characteristic of both map task and reduce task. But these tasks won't change the behavior of shuffle. To avoid ambiguity, in the following paper, we will only use term of map task to represent those who produce shuffle output, and reduce task to represent those who consume shuffle output.
% \begin{figure}
% 	\centering
% 	\includegraphics[width=\linewidth]{fig/shuffle_process}
% 	\caption{Shuffle Overview}
% 	\label{fig:shuffle_process}
% \end{figure}

% \textbf{Overview of shuffle process}

% \subsubsection{Overview of shuffle process}
% Each map task partitions the result data (key, value pair) into several buckets according to the partition function (e.g., hash). 
% The total number of buckets equals the number of reduce tasks in the successive step.
% % shuffle data is produced by \textit{data partition}. For \textit{data partition}, 
% The shuffle process can be further split into two parts: \textit{shuffle write} and \textit{shuffle read}. 
% Shuffle write starts at the end of a map task and writes the partitioned map output data to local persistent storage. 
% Shuffle read starts at the beginning of a reduce task and fetches the partitioned data from remote as its input. 

%In short, shuffle is loosely coupled with application context and it's I/O intensive.
% \textbf{Impact of shuffle process}

% \subsubsection{Impact of shuffle process}
% {\color{black}
% Shuffle is I/O intensive, which might introduce a significant latency to the application. 
% Reports show that $60\%$ of MapReduce jobs at Yahoo! and $20\%$ at Facebook are shuffle-heavy workloads \cite{shufflewatcher}. 
% For those shuffle-heavy jobs, the shuffle latency may even dominate the completion time of jobs.
% For those shuffle-heavy jobs, the shuffle latency may even dominate Job Completion Time (JCT).
% For instance, a MapReduce trace analysis from Facebook shows that shuffle accounts for $33\%$ JCT on average, up to $70\%$ in shuffle-heavy jobs \cite{managing}.
% }

\subsection{Observations} \label{observation}
% Of course, shuffle is unavoidable in a DAG computing process. 
Can we mitigate or even remove the overhead of shuffle?
To analyze the design and implementation of shuffle, we run some Spark's \textit{GroupByTest} on a 5-node m4.xlarge EC2 cluster.
% To find the answers, we ran some typical Spark applications on a 5-node \texttt{m4.xlarge} EC2 cluster and analyzed the design and implementation of shuffle in some DAG frameworks.
Figure \ref{fig:util} shows the hardware utilization trace of one node of the experiment.
% Here we present the hardware utilization trace of one node running Spark's \textit{GroupByTest} in Figure \ref{fig:util} as an example. 
This job has 2 rounds of tasks for each node.
The \textit{Map Execution} is marked from the launch time of the first map task to the execution end time of the last one. 
The \textit{Shuffle Write} is marked from the beginning of the first shuffle write in the map stage. 
The \textit{Shuffle Read and Reduce Execution} is marked from the launch time of the first reduce task.
% Figure \ref{fig:util} reveals the performance information of two stages that are connected by shuffle. By analyzing the trace with Spark source code \cite{sparksource}, we propose the following observations.
% \begin{figure*}
% 	\includegraphics[width=\textwidth]{fig/util}
% 	\caption{CPU utiliazation and I/O throughput of a node during a Spark single shuffle application}
% 	\label{fig:util}
% \end{figure*}
\subsubsection{Coarse Granularity Resource Allocation}
{\color{black}
As shown in Figure \ref{fig:util}, the network transfer of shuffle data introduces an explicit I/O delay during \textit{shuffle read}.
Both \textit{shuffle write} and \textit{shuffle read} occupy the slot without significantly involving CPU.
The current coarse slot-task mapping results in an imbalance between task's resource demand and slot allocation thus decreasing the resource utilization. 

% {\color{red}
% When a slot is assigned to a task, it will not be released until the task completes (i.e., the end of shuffle write in Figure \ref{fig:util}). 
% On the reduce side, the network transfer of shuffle data introduces an explicit I/O delay during shuffle read (i.e., the beginning of shuffle read and execution in Figure \ref{fig:util}). 
% Meanwhile, both shuffle write and shuffle read occupy the slot without significantly involving CPU as presented in Figure \ref{fig:util}. 
% The current coarse slot-task mapping results in an imbalance between task's resource demand and slot allocation thus decreasing the resource utilization. 
% Unfortunately this defect exists not only in Spark but also Hadoop MapReduce and Tez. 
% A finer granularity resource allocation scheme should be provided to reduce these delays. 
% }

\subsubsection{Synchronized Shuffle Read}

The synchronized shuffle read requests cause several bursts of network traffic, which may result in network congestion and further slow down the network transfer.

% {\color{red}
% Almost all reduce tasks start shuffle read simultaneously. 
% The synchronized shuffle read requests cause a burst of network traffic. 
% As shown in Figure \ref{fig:util}, 
% the data transfer causes a high demand of network bandwidth, which may result in network congestion and further slow down the network transfer.
% It also happens in other frameworks that follow Bulk Synchronous Parallel (BSP) paradigm, such as Hadoop MapReduce, Dryad \cite{dryad}, etc.
% }

% The previous work \cite{coflow, managing} also proves that the network transfer can introduce significant overhead in DAG computing.

\subsubsection{Inefficient Persistent Storage Operation}
Both shuffle write and shuffle read are tightly coupled with task execution, which results in a blocking I/O operation.
This blocking I/O operation may introduce significant latency, especially in an I/O performance bounded cluster.
Besides, most of the DAG frameworks store shuffle data on disks (e.g., Spark, Hadoop MapReduce, Dryad, etc).
We argue that the memory capacity is large enough to store the short-living shuffle data during shuffle phases since several memory-based distributed storage systems have been proposed \cite{tachyon, ramcloud}.

% At first, both shuffle write and read are tightly coupled with task execution, which results in a blocking I/O operation. 
% This blocking I/O operation along with synchronized shuffle read may introduce significant latency, especially in an I/O performance bounded cluster.
% Besides, the legacy of storing shuffle data on disk is inefficient in modern clusters with large memory. 
% Compared to input dataset, the size of shuffle data is relatively small. 
% For example, the shuffle size of Spark Terasort is less than $25\%$ of input data. 
% The data reported in \cite{makingsense} also show that the amount of data shuffled is less than the input data by as much as $10\%-20\%$. 
% On the other hand, memory based distributed storage systems have been proposed \cite{tachyon, ramcloud} to move data back to memory, 
% but most of the DAG frameworks still store shuffle data on disks (e.g., Spark, Hadoop MapReduce, Dryad \cite{dryad}, etc).
% We argue that the memory capacity is large enough to store the short-living shuffle data with cautious management.

\subsubsection{Multi-round Tasks Execution}\label{multi}
Both experience and DAG framework manuals (e.g., Hadoop and Spark) recommend that multi-round execution of each stage will benefit the performance of applications.
Since the shuffle data becomes available when map tasks complete, 
and the network is idle during the map stage (\textit{Network TX} during map stage in Figure \ref{fig:util}), 
we propose an optimization that starts \textit{shuffle read} ahead of reduce stage to overlap the I/O operations in multi-round map tasks. 
% To achieve this optimization:
% \begin{itemize}
% 	\item Shuffle process should be decoupled from task execution to achieve a fine granularity scheduling scheme.
% 	\item Reduce tasks should be pre-scheduled without launching to achieve shuffle data pre-fetching.
% 	\item Shuffle process should be taken over and managed outside DAG frameworks to achieve a cross-framework optimization.
% \end{itemize}
% In the following section, we elaborate the methodologies to achieve three design goals.
}
