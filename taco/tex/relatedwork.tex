\section{Related Work}

{\color{blue}
\textbf{Modeling}: Most representative DAG computing frameworks use similar \textit{Bulk Synchronize Parallel} (BSP)\cite{valiant1990bridging} model to control data synchronization in each computing phase, i.e. \textit{stage} in Spark, \textit{superstep} in Pregel\cite{malewicz2010pregel} and so on. 
% (Hadoop MapReduce can also be considered as a special case of only one superstep in BSP model). 
% In the process of optimizing the shuffle phases between adjacent computing phases, we design a performance model to assist in analyzing computing process.
% There are also lots of performance models for MapReduce have been proposed, such as \cite{verma2011aria, khan2016hadoop, herodotou2011hadoop, chen2014cresp, farhat2016stochastic}.
Verma et al. \cite{verma2011aria} proposed the ARIA model to estimate the required resources based on the job information, the amount of input data and a specified soft deadline.
Khan et al. \cite{khan2016hadoop} proposed the HP model which extends the ARIA model. The HP model adds scaling factors and uses a simple linear regression to estimate the job execution time on larger datasets.
In \cite{herodotou2011hadoop}, Herodotou proposed a detailed set of mathematical performance models for describing the five phases of a MapReduce job abd combine them into an overall MapReduce job model.
% The execution is separated into the phases: Read, Map, Collect, Spill, Merge, Shuffle, Merge, and Reduce. 
% The set of performance models describe each above-mentioned phases and combine into an overall MapReduce job model. 
Chen et al. \cite{chen2014cresp} proposed the CRESP model which is a cost model that estimates the performance of a job then 
provisions the resources for the job.
Farshid Farhat et al.\cite{farhat2016stochastic} proposed a closed-form queuing model which focus on stragglers and try to optimize them. 
% The proposed model is parameterized by the task inter-arrival times to the mappers, the delayed tailed service times, and the number of mappers, which is used to optimize the stragglers. 
However, the above models are not able to accurately describe the overhead caused by the shuffle process under different scheduling strategies. 
% Different from these models, our FRQ model quantifies computing and I/O resources and visualizes them in the time dimension. 
The FRQ model focuses on describing the overhead caused by the shuffle process in different scheduling strategies, which satisfies our demand.
}

\textbf{Pre-scheduling}: Slow-start from Apache Hadoop MapReduce is a classic approach to handle shuffle overhead. 
Starfish \cite{starfish} gets sampled data statics for self-tuning system parameters (e.g. slow-start, etc). 
DynMR \cite{dynmr} dynamically starts reduce tasks in late map stage. 
All of them have the explicit I/O time in occupied slots. 
SCache instead starts shuffle pre-fetching without consuming slots. 
iShuffle \cite{guo2017ishuffle} decouples shuffle from reducers and designs a centralized shuffle controller. 
But it can neither handle multiple shuffles nor schedule multiple rounds of reduce tasks. 
iHadoop \cite{ihadoop} aggressively pre-schedules tasks in multiple successive stages to start fetching shuffle. 
But we have proved that randomly assign tasks may hurt the overall performance in Section \ref{randomassign}. 
Different from these works, SCache pre-schedules multiple shuffles without breaking load balancing. 

%  by combining DAG information and heuristic algorithms.
\textbf{Delay-scheduling}: Delay Scheduling \cite{delay} delays tasks assignment to get better data locality, which can reduce the network traffic. 
ShuffleWatcher \cite{shufflewatcher} delays shuffle fetching when the network is saturated. 
At the same time, it achieves better data locality. 
Both Quincy \cite{quincy} and Fair Scheduling \cite{preemptive} can reduce shuffle data by optimizing data locality of map tasks. 
But all of them cannot mitigate explicit I/O in both map and reduce tasks. 
In addition, their optimizations fluctuate under different network performances and data distributions, whereas SCache can provide a stable performance gain by shuffle data pre-fetching and in-memory caching.

\textbf{Network layer optimization}: Varys \cite{varys} and Aalo \cite{aalo} provide the network layer optimization for shuffle transfer. 
Though the efforts are limited throughout whole shuffle process, they can be easily applied on SCache to further improve the performance.