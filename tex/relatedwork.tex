\section{Related Work}

{\color{blue}
\textbf{Modeling}: Most well-known DAG computing frameworks use similar \textit{Bulk Synchronize Parallel}(BSP)\cite{valiant1990bridging} model to control data synchronization in each computing phase, i.e. \textit{stage} in Spark, \textit{superstep} in Pregel\cite{malewicz2010pregel} and so on(Apache Hadoop Madpreduce\footnote{http://hadoop.apache.com/} can also be considered as a special case of only one superstep in BSP model). 
In the process of optimizing the shuffle phases between adjacent computing phases, we design a performance model to assist in analyzing computing process.
Inspired by \cite{verma2011aria}, \cite{herodotou2011hadoop}, and \cite{polo2010performance}, we present \textit{Framework Resources Quantification}(FRQ) model to describe the performance of DAG frameworks.
In \cite{verma2011aria}, the author designs a model and divides execution into three parts: Map, Shuffle and Reduce. And then the author uses a greedy algorithm to roughly calculate the max, min and mean execution time of Map and Reduce phases.
In \cite{herodotou2011hadoop}, the author presents a detailed set of mathematical performance models for describing the execution of a MapReduce job. The execution is seperated into the phases: Read, Map, Collect, Spill, Merge, Shuffle, Merge, and Reduce. The performance models describe each above-mentioned phases and combine into a overall Mapreduce job model. 
However, none of the above models meet our requirements for analyzing shuffle.
The model in \cite{verma2011aria} is not able to accurately describe the overhead caused by shuffle under different scheduling strategies. The model in \cite{herodotou2011hadoop} calculates overhead of various phases including shuffle, but most of them are redundant. 
Different from these models, our FRQ model quantifies computing and I/O resources and displays them in time dimension. FRQ model focuses on describing the overhead caused by shuffle in different scheduling strategies, which satisfy our demand.
}

\textbf{Pre-scheduling}: Slow-start from Apache Hadoop MapReduce is a classic approach to handle shuffle overhead. 
Starfish \cite{starfish} gets sampled data statics for self-tuning system parameters (e.g. slow-start, etc). 
DynMR \cite{dynmr} dynamically starts reduce tasks in late map stage. 
All of them have the explicit I/O time in occupied slots. 
SCache instead starts shuffle pre-fetching without consuming slots. 
iShuffle \cite{ishuffle} decouples shuffle from reducers and designs a centralized shuffle controller. 
But it can neither handle multiple shuffles nor schedule multiple rounds of reduce tasks. 
iHadoop \cite{ihadoop} aggressively pre-schedules tasks in multiple successive stages to start fetching shuffle. 
But we have proved that randomly assign tasks may hurt the overall performance in Section \ref{randomassign}. 
Different from these works, SCache pre-schedules multiple shuffles without breaking load balancing. 

%  by combining DAG information and heuristic algorithms.
\textbf{Delay-scheduling}: Delay Scheduling \cite{delay} delays tasks assignment to get better data locality, which can reduce the network traffic. 
ShuffleWatcher \cite{shufflewatcher} delays shuffle fetching when network is saturated. 
At the same time, it achieves better data locality. 
Both Quincy \cite{quincy} and Fair Scheduling \cite{preemptive} can reduce shuffle data by optimizing data locality of map tasks. 
But all of them cannot mitigate explicit I/O in both map and reduce tasks. 
In addition, their optimizations fluctuate under different network performances and data distributions, whereas SCache can provide a stable performance gain by shuffle data pre-fetching and in-memory caching.

\textbf{Network layer optimization}: Varys \cite{varys} and Aalo \cite{aalo} provide the network layer optimization for shuffle transfer. 
Though the efforts are limited throughout whole shuffle process, they can be easily applied on SCache to further improve the performance.